# -*- coding: utf-8 -*-
"""model ผปก.

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189DQzK28j7LbxIaBJhRT5IYmBhlGJu1x

#**Installation**
"""

!pip install -U sentence-transformers
!pip install attacut
!pip install umap-learn
!pip install "umap-learn[plot]"

"""#**IMPORT**  **LIBRARIES**



*  NumPy: จัดการ array
*   pandas : จัดการ data frame และทำความสะอาดชุดข้อมูล
* tensorboard : พล็อต UMAP
* Tokenizer : ตัดข้อความเป็นคำศัพท์ขอ
* TfidfVectorizer  : ใช้เริ่มต้นวิเคราะห์จาก document
* io : การอ่าน string ให้เหมือนอ่านมาจากไฟล์


"""

from sentence_transformers import SentenceTransformer, util
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
import numpy as np
import io

from attacut import tokenize, Tokenizer

import umap
import umap.plot
from sklearn.datasets import load_digits

"""**Define** **Tokenizer** **Function**"""

def tokenizer_text(sentence):
    words = tokenize(sentence)
    space_sentence = " ".join(words)
    return space_sentence

"""#**Preprocess**

* เลือกไฟล์ dataset


"""

from google.colab import files
uploaded = files.upload()

df = pd.read_excel(io.BytesIO(uploaded.get('dataset314.xlsx')))
df

"""

*   ใช้ fillna เติมข้อมูลที่เป็น Nan ด้วยเครื่องหมาย " - "


"""

df = df.fillna('-')

df.info()

"""

*   เปลี่ยน type ของcolumn ประกอบธุรกิจ , product_type , กลุ่มผลิตภัณฑ์ และ bussimedd_type ให้เป็น string เพื่อจะได้สามารถทำให้เป็น เวกเตอร์ได้


"""

#df['TSIC CODE'] = df['TSIC CODE'].astype('object')
df['ประกอบธุรกิจ'] = df['ประกอบธุรกิจ'].astype('string')
df['product_type'] = df['product_type'].astype('string')
df['กลุ่มผลิตภัณฑ์'] = df['กลุ่มผลิตภัณฑ์'].astype('string')
df['bussiness_type'] = df['bussiness_type'] .astype('string')

"""

*   นำ column   product_type กับ กลุ่มผลิตภัณฑ์ และ bussiness_typee กับ ประกอบธุรกิจ มารวมกันเพื่อที่จะนำไปสกัดคำ

"""

df['details'] =  df['product_type']+ df['กลุ่มผลิตภัณฑ์']
df['bussinees'] = df['bussiness_type'] + df['ประกอบธุรกิจ'] 
df['product_bussiness'] =  df['product_type']+ df['กลุ่มผลิตภัณฑ์']+df['bussiness_type'] + df['ประกอบธุรกิจ'] 
df['All'] = df['details'].apply(tokenizer_text)
df['bussinees_grop'] = df['bussinees'].apply(tokenizer_text)
df['bussinees_product'] = df['product_bussiness'].apply(tokenizer_text)
df['target'] = df['TSIC_Group'].astype('category').cat.codes

#df

"""**TF-IDF**
เป็นสถิติเชิงตัวเลขอย่างง่ายที่ใช้ในการพิจารณา ความเกี่ยวข้องของข้อความที่สัมพันธ์กับคำในคำค้นหา 
ใช้เทคนิคการคัดแยกคำตามความสำคัญโดยการให้น้ำหนักคำในแต่ละคำ
* เป็นหนึ่งในปัญหาพื้นฐานในการดึงข้อมูล (Information Retrieving) ซึ่งจะ
เรียงลำดับ document ว่าอันไหนใกล้เคียงสุด (ดูว่าประโยคที่รับเข้ามาใหม่คล้ายกับประโยคไหนใน document บ้าง) โดย กำหนดคะแนน [0, 1] ในทุกๆ document
คะแนนจะวัดว่ามันตรงกันมากน้อยเท่าไร

*   Term Frequency(TF) การดูความถี่ของคำที่ปรากฏใน Document
*   Document Frequency(DF) ทำหน้าที่หาคำที่มีความเฉพาะ แปลกใหม่ ที่่ปรากฏใน Document ทั้งหมด 
* Inverse  Document Frequency (IDF )  การคำนวณค่าน้ำหนักความสำคัญของแต่ละคำในทุก Document

#**Embedding** **with** **TFIDF**

Tf-idf เอาไว้หา word similarity (ความเหมือนกันของคำ) คำจะถูกแปลงเป็น Vector ที่แทนด้วยความถี่ ดังนั้นจึงเกิด Sparse matrix (Matrix ที่ประกอบไปด้วยเลข 0 จำนวนมาก) 
Word embedding จึงสามารถมาทดแทนช่องว่างตรงนี้ได้
"""

y = df['target'].values

vectorizer = TfidfVectorizer()
X_tfidf = vectorizer.fit_transform(df['All'].values)

X_tfidf.toarray()

"""#**เทคนิคTfidf** **ของกลุ่มผลิตภัณฑ์**
#**umap** **&** **plot** **interactive** 

"""

mapper = umap.UMAP().fit(X_tfidf)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
#f = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5,)
umap.plot.show(p)

tfidf = vectorizer.fit_transform(df['bussinees_grop'].values)

tfidf.toarray()

"""#**เทคนิคTfidf** **ของกลุ่มธุรกิจ**
#**umap** **&** **plot** **interactive** 

"""

mapper = umap.UMAP().fit(tfidf)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
#bussiness = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5,)
umap.plot.show(p)

product_bussiness = vectorizer.fit_transform(df['bussinees_product'].values)

product_bussiness.toarray()

"""#**เทคนิคTfidf** **ของกลุ่มธุรกิจกับกลุ่มผลิตภัณฑ์**
#**umap** **&** **plot** **interactive** 

"""

mapper = umap.UMAP().fit(product_bussiness)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
bussiness = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5,)
umap.plot.show(p)

"""* แบ่ง training sets เป็น 75 และ testing sets เป็น 25 
* ใช้ TF*IDF Vectorizer
"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train, Y_test = train_test_split(df['All'], df['target'], test_size=0.25, random_state=30)
tf_x_train = vectorizer.fit_transform(X_train)
tf_x_test = vectorizer.transform(X_test)

"""การใช้ SVM ใน classification"""

from sklearn.svm import LinearSVC
clf = LinearSVC(random_state=0)

"""Fitting the Training data into model"""

clf.fit(tf_x_train,Y_train)

"""Predicting the Test data"""

y_test_pred=clf.predict(tf_x_test)

"""Analyzing the results"""

from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from sklearn.metrics import classification_report
matrix = confusion_matrix(Y_test, y_test_pred, labels=[1,0])
print('confusion matrix :\n '  ,matrix)
tp , fn, fp, tn =confusion_matrix(Y_test, y_test_pred, labels=[1,0]).reshape(-1)
print('ouput valuse : \n'  , tp , fn, fp, tn )

matrix=classification_report(Y_test, y_test_pred, labels=[1,0])
print('classification report :\n '  , matrix)

"""ใช้ the SVM classifier จะได้ค่า  accuracy ที่ 73.65 %

#**Embedding** **with** **xlm** **roberta** **base**


* RoBERTa ใช้ตัวตัดคำย่อย SentencePiece ซึ่งมีข้อดีตรงที่ไม่ต้องการการตัดคำล่วงหน้า (pretokenization) เหมือนตัวตัดคำย่อยอื่น เช่น BPE หรือ WordPiece จึงเหมาะกับภาษาที่กฎการตัดคำไม่ตายตัวอย่างภาษาไทย และสามารถปรับขนาดเป็น 100 ภาษา
"""

model = SentenceTransformer('xlm-roberta-base')

"""**encoder** จะเทรนโดยการให้ประโยคเริ่มต้นและประโยควิบัติ(เช่น เว้นว่างคำบางคำในประโยค) และเป้าหมายของโมเดลคือหาวิธีสร้างประโยคเริ่มต้นให้ดีกว่าเดิม โมเดล encoder เหมาะที่จะต้องการความเข้าใจประโยคทั้งประโยค เช่น การแยกแยะประโยค, การระบุคำเฉพาะในประโยค (รวมถึงการแยกแยะประเภทคำ), และการสกัดคำถามคำตอบ"""

X_roberta = model.encode(df['details'].values)

X_roberta

"""#**เทคนิคRoberta** **ของกลุ่มผลิตภัณฑ์**
#**umap** **&** **plot** **interactive** 

"""

mapper = umap.UMAP().fit(X_roberta)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
#f = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5)
umap.plot.show(p)

roberta = model.encode(df['bussinees_grop'].values)

roberta

"""#**เทคนิคRoberta** **ของกลุ่มธุรกิจ**
#**umap** **&** **plot** **interactive** 
"""

mapper = umap.UMAP().fit(roberta)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
#f = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5)
umap.plot.show(p)

bussinees_product = model.encode(df['bussinees_product'].values)

bussinees_product

"""#**เทคนิคRoberta** **ของกลุ่มธุรกิจกับกลุ่มผลิตภัณฑ์**
#**umap** **&** **plot** **interactive** 
"""

mapper = umap.UMAP().fit(bussinees_product)
hover_df = pd.DataFrame(df, columns=['TSIC_Group'])
#f = umap.plot.points(mapper, labels=hover_df['TSIC_Group'])

umap.plot.output_notebook()
p = umap.plot.interactive(mapper, labels=hover_df['TSIC_Group'],hover_data = df, point_size=5)
umap.plot.show(p)

"""แสดง label ของ rows ซึ่ง 


- 0 คือ 32111
- 1 คือ 32112
- 2 คือ 32113
- 3 คือ 46492
- 4 คือ 47732
- 5 คือ other	
 
"""

df_label = df[['TSIC_Group', 'target']]
df_label = df_label.rename(columns={'TSIC_Group': 'TSIC Code', 'target': 'label' })
df_label

"""**ทดสอบ** **check** **ความสัมพันธ์**"""

sentences = ['การทำเครื่องประดับจากเพชรพลอย และหินมีค่าทุกชนิด และทำแว่นสายตา', # 0
              'ผลิต รับจ้างทำเครื่องประดับอัญมณีแท้และเทียม ซื้อขาย รับซื้อคืน ทอง นาก เงิน เพชร พลอย อัญมณี', # 1
             'ผลิตและจำหน่ายกรอบพระเลี่ยมทอง', # 2
           'นำเข้า-ส่งออกเครื่องประดับอัญมณี', # 3 
             'แมวใส่เสื้อสีแดง',   # 4
             'cat wears red hat',  #  5
          ]

#Encode all sentences
embeddings = model.encode(sentences)

#Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

#Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim)-1):
    for j in range(i+1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

#Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:10]:
#     print("{} \t |||| {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
    print("{} \t  {} \t {:.4f}".format(i, j, cos_sim[i][j]))

